\relax 
\citation{Szkely2013}
\citation{RizzoVariance}
\citation{RizzoClustering}
\citation{Kgroups}
\citation{Szkely2013}
\citation{Lyons}
\citation{Sejdinovic2013}
\citation{Lloyd,MacQueen,Forgy}
\citation{Lloyd}
\citation{Smola,Girolami}
\citation{Mercer}
\citation{Girolami}
\citation{Dhillon2,Dhillon}
\citation{Lloyd}
\citation{Filippone}
\citation{Kgroups}
\citation{Dhillon,Dhillon2}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{sec:introduction}{{1}{1}}
\citation{Hartigan}
\citation{Kgroups}
\citation{Telgarsky,Slonin}
\citation{Szkely2013}
\citation{Sejdinovic2013}
\citation{Szkely2013}
\citation{Sejdinovic2013}
\citation{Aronszajn}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}}
\newlabel{sec:background}{{2}{2}}
\newlabel{eq:energy}{{1}{2}}
\newlabel{eq:energy2}{{2}{2}}
\newlabel{eq:negative_type}{{2}{2}}
\MT@newlabel{eq:energy}
\newlabel{eq:energy3}{{3}{2}}
\MT@newlabel{eq:energy3}
\citation{Gretton2012}
\citation{Berg1984}
\citation{Sejdinovic2013}
\citation{Sejdinovic2013}
\citation{Szkely2013}
\citation{Szkely2013}
\citation{Kgroups}
\newlabel{eq:mmd}{{4}{3}}
\newlabel{eq:mmd2}{{5}{3}}
\MT@newlabel{eq:mmd}
\MT@newlabel{eq:mmd2}
\newlabel{eq:kernel_semimetric}{{6}{3}}
\newlabel{eq:gen_kernel}{{7}{3}}
\MT@newlabel{eq:energy3}
\MT@newlabel{eq:gen_kernel}
\MT@newlabel{eq:energy3}
\MT@newlabel{eq:mmd2}
\newlabel{eq:Erho}{{2}{3}}
\MT@newlabel{eq:mmd}
\MT@newlabel{eq:energy3}
\newlabel{eq:g_def}{{8}{3}}
\newlabel{eq:within}{{9}{3}}
\newlabel{eq:between}{{10}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Clustering Based on Energy Statistics}{3}}
\newlabel{sec:clustering_theory}{{3}{3}}
\MT@newlabel{eq:between}
\MT@newlabel{eq:within}
\newlabel{th:minimize}{{1}{3}}
\MT@newlabel{eq:between}
\newlabel{eq:minimize}{{11}{3}}
\MT@newlabel{eq:within}
\MT@newlabel{eq:within}
\MT@newlabel{eq:between}
\citation{Malik,NgJordan}
\MT@newlabel{eq:minimize}
\MT@newlabel{eq:minimize}
\MT@newlabel{eq:minimize}
\newlabel{eq:kernel_matrix}{{12}{4}}
\newlabel{eq:label_matrix}{{13}{4}}
\MT@newlabel{eq:minimize}
\newlabel{th:qcqp2}{{2}{4}}
\MT@newlabel{eq:minimize}
\newlabel{eq:qcqp2}{{14}{4}}
\MT@newlabel{eq:kernel_matrix}
\MT@newlabel{eq:gen_kernel}
\MT@newlabel{eq:g_def}
\MT@newlabel{eq:within}
\newlabel{eq:W2}{{15}{4}}
\MT@newlabel{eq:W2}
\newlabel{eq:max_prob}{{16}{4}}
\MT@newlabel{eq:kernel_matrix}
\MT@newlabel{eq:label_matrix}
\MT@newlabel{eq:max_prob}
\MT@newlabel{eq:label_matrix}
\newlabel{eq:qcqp}{{17}{4}}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:qcqp}
\MT@newlabel{eq:qcqp}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:qcqp2}
\newlabel{eq:relaxed}{{3}{4}}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:energy2}
\MT@newlabel{eq:kernel_semimetric}
\citation{Lloyd}
\citation{Dhillon2,Dhillon}
\citation{Dhillon2,Dhillon}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:kernel_matrix}
\newlabel{eq:kernel_kmeans}{{18}{5}}
\MT@newlabel{eq:kernel_kmeans}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:minimize}
\MT@newlabel{eq:kernel_kmeans}
\newlabel{th:kernel_kmeans}{{3}{5}}
\MT@newlabel{eq:minimize}
\MT@newlabel{eq:kernel_kmeans}
\MT@newlabel{eq:qcqp2}
\newlabel{eq:J}{{19}{5}}
\MT@newlabel{eq:kernel_kmeans}
\MT@newlabel{eq:max_prob}
\MT@newlabel{eq:within}
\MT@newlabel{eq:kernel_kmeans}
\MT@newlabel{eq:kernel_kmeans}
\MT@newlabel{eq:minimize2}
\MT@newlabel{eq:kernel_kmeans}
\MT@newlabel{eq:kernel_kmeans}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:qcqp2}
\@writefile{toc}{\contentsline {section}{\numberline {4}A Weighted Generalization}{5}}
\newlabel{sec:weighted}{{4}{5}}
\newlabel{eq:g_def2}{{20}{5}}
\MT@newlabel{eq:g_def2}
\MT@newlabel{eq:within}
\MT@newlabel{eq:between}
\newlabel{eq:minimize2}{{21}{5}}
\MT@newlabel{eq:g_def2}
\newlabel{eq:weighted_matrices}{{22}{5}}
\newlabel{th:qcqp3}{{4}{5}}
\MT@newlabel{eq:minimize2}
\newlabel{eq:qcqp3}{{23}{5}}
\MT@newlabel{eq:kernel_matrix}
\MT@newlabel{eq:gen_kernel}
\MT@newlabel{eq:minimize2}
\citation{Dhillon2,Dhillon}
\citation{Kernighan,Malik,Chan,Yu}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Connection with Graph Partitioning}{6}}
\newlabel{eq:assoc}{{24}{6}}
\newlabel{eq:cut}{{25}{6}}
\MT@newlabel{eq:within}
\MT@newlabel{eq:between}
\MT@newlabel{eq:assoc}
\MT@newlabel{eq:cut}
\MT@newlabel{eq:assoc}
\MT@newlabel{eq:label_matrix}
\MT@newlabel{eq:weighted_matrices}
\MT@newlabel{eq:qcqp3}
\MT@newlabel{eq:gen_kernel}
\newlabel{eq:metric_graphs}{{4.1}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Two-Class Problem in One Dimension}{6}}
\newlabel{sec:twoclass}{{5}{6}}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:g_def}
\newlabel{eq:g_ind}{{5}{6}}
\newlabel{eq:g1d}{{5}{6}}
\citation{scikit-learn}
\citation{Vassilvitskii}
\citation{Dhillon2,Dhillon}
\newlabel{eq:w1d}{{26}{7}}
\MT@newlabel{eq:w1d}
\MT@newlabel{eq:w1d}
\newlabel{algo1d}{{1}{7}}
\MT@newlabel{eq:minimize}
\MT@newlabel{eq:w1d}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces   $\mathcal  {E}^{1D}$-clustering algorithm to find local solutions to the optimization problem \MT_extended_eqref:n  {eq:minimize} for a two-class problem in one dimension. }}{7}}
\newlabel{eq:accuracy}{{27}{7}}
\MT@newlabel{eq:accuracy}
\newlabel{eq:two_normal}{{28}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   $\mathcal  {E}^{1D}$-clustering versus $k$-means and GMM. (a,b) We plot the mean accuracy \MT_extended_eqref:n  {eq:accuracy} over $100$ Monte Carlo trials, versus the number of sampled points. Error bars are standard error. The dashed line indicates Bayes accuracy ($\approx 0.88$ in both cases). (a) Clustering results for data normally distributed as in \MT_extended_eqref:n  {eq:two_normal}. (b) Data lognormally distributed as in \MT_extended_eqref:n  {eq:two_lognormal}. (c) Density estimation of each component in the mixture \MT_extended_eqref:n  {eq:two_normal} after clustering $1000$ sampled points using the three algorithms, compared to the ground truth. (d) The same but for lognormal data \MT_extended_eqref:n  {eq:two_lognormal}. }}{7}}
\newlabel{fig:1d}{{1}{7}}
\MT@newlabel{eq:accuracy}
\MT@newlabel{eq:two_normal}
\MT@newlabel{eq:two_lognormal}
\MT@newlabel{eq:two_normal}
\MT@newlabel{eq:two_lognormal}
\newlabel{eq:two_lognormal}{{29}{7}}
\MT@newlabel{eq:two_lognormal}
\@writefile{toc}{\contentsline {section}{\numberline {6}Iterative Algorithms}{7}}
\newlabel{sec:algo}{{6}{7}}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:max_prob}
\newlabel{eq:maxQ}{{30}{7}}
\citation{Lloyd}
\citation{Dhillon2,Dhillon}
\citation{Hartigan}
\newlabel{eq:costxij}{{6}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Lloyd's Method for Energy Clustering}{8}}
\MT@newlabel{eq:J}
\newlabel{eq:Jell}{{31}{8}}
\MT@newlabel{eq:Jell}
\MT@newlabel{eq:Jell}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Hartigan's Method for Energy Clustering}{8}}
\MT@newlabel{eq:maxQ}
\MT@newlabel{eq:qcqp2}
\newlabel{kmeans_algo}{{2}{8}}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:maxQ}
\MT@newlabel{eq:Jell}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces  $\mathcal  {E}^{L}$-clustering is Lloyd's method for energy clustering, which is precisely kernel $k$-means algorithm, with the kernel induced by energy statistics. This procedure finds local solutions to the optimization problem \MT_extended_eqref:n  {eq:qcqp2}. }}{8}}
\MT@newlabel{eq:maxQ}
\newlabel{eq:changeQ}{{32}{8}}
\MT@newlabel{eq:maxQ}
\citation{Telgarsky}
\citation{Telgarsky}
\citation{Telgarsky}
\citation{Telgarsky}
\citation{Slonin}
\citation{Slonin}
\citation{scikit-learn}
\citation{Vassilvitskii}
\citation{Malik}
\newlabel{algo}{{3}{9}}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:maxQ}
\MT@newlabel{eq:changeQ}
\newlabel{stepmove}{{6.2}{9}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces  $\mathcal  {E}^H$-clustering is Hartigan's method for energy clustering. This algorithm finds local solutions to the optimization problem \MT_extended_eqref:n  {eq:qcqp2}. The steps $6$, $12$ and $13$ are different than $\mathcal  {E}^L$-clustering described in Algorithm\nobreakspace  {}2\hbox {}. }}{9}}
\newlabel{noempty}{{1}{9}}
\newlabel{diffmean}{{2}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Numerical Experiments}{9}}
\newlabel{sec:numerics}{{7}{9}}
\newlabel{eq:rho_alpha}{{33}{9}}
\newlabel{eq:rho_tilde}{{34}{9}}
\newlabel{eq:rho_hat}{{35}{9}}
\citation{Telgarsky}
\MT@newlabel{eq:kernel_semimetric}
\MT@newlabel{eq:energy}
\MT@newlabel{eq:accuracy}
\newlabel{eq:gauss1}{{36}{10}}
\newlabel{eq:gauss2}{{37}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   Pair plots for the first $5$ dimensions. (a) Data normally distributed as in \MT_extended_eqref:n  {eq:gauss1}. (b) Data normally distributed as in \MT_extended_eqref:n  {eq:gauss2}. We sample $200$ points for both cases. We can see that there is a considerable overlap between the clusters. }}{10}}
\newlabel{fig:pairsplot}{{2}{10}}
\MT@newlabel{eq:gauss1}
\MT@newlabel{eq:gauss2}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Comparison of $\mathcal  {E}^H$-clustering, $\mathcal  {E}^L$-clustering (kernel $k$-means), spectral clustering, $k$-means and GMM in high dimensional Gaussian settings. We plot the mean accuracy versus the number of dimensions, with error bars indicating standard error from $100$ Monte Carlo runs. (a) Data normally distributed as in \MT_extended_eqref:n  {eq:gauss1}, with Bayes accuracy $\approx 0.86$, over the range $D \in [10,200]$. (b) Data normally distributed as in \MT_extended_eqref:n  {eq:gauss2}, with Bayes accuracy $\approx 0.95$, over the range $D \in [10, 700]$. }}{10}}
\newlabel{fig:gauss}{{3}{10}}
\MT@newlabel{eq:gauss1}
\MT@newlabel{eq:gauss2}
\citation{Sapiro}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   $\mathcal  {E}^H$-clustering with metrics \MT_extended_eqref:n  {eq:rho_alpha} and \MT_extended_eqref:n  {eq:rho_tilde} versus $k$-means and GMM. In both settings Bayes accuracy is $\approx 0.9$. We show average accuracy (error bars are standard error) versus number of points for $100$ Monte Carlo trials. (a,b) Gaussian mixture \MT_extended_eqref:n  {eq:20gauss}. (c,d) Lognormal mixture \MT_extended_eqref:n  {eq:20loggauss}. The plots in (c) and (d) consider the difference in accuracy between $\mathcal  {E}^H$ versus $\mathcal  {E}^L$ (kernel $k$-means) and spectral clustering, with metric $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \rho $}\mathaccent "0365{\rho }_1$. }}{11}}
\newlabel{fig:consist}{{4}{11}}
\MT@newlabel{eq:rho_alpha}
\MT@newlabel{eq:rho_tilde}
\MT@newlabel{eq:20gauss}
\MT@newlabel{eq:20loggauss}
\MT@newlabel{eq:gauss2}
\newlabel{eq:20gauss}{{38}{11}}
\MT@newlabel{eq:rho_alpha}
\MT@newlabel{eq:rho_tilde}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  (a) Parallel cigars. (b) Two concentric circles with noise. (c) Three concentric circles with noise. (d) MNIST handwritten digits. Clustering results are in Table\nobreakspace  {}1\hbox {} and Table\nobreakspace  {}2\hbox {}. }}{11}}
\newlabel{fig:other}{{5}{11}}
\newlabel{eq:20loggauss}{{39}{11}}
\MT@newlabel{eq:rho_alpha}
\MT@newlabel{eq:rho_tilde}
\MT@newlabel{eq:rho_hat}
\newlabel{eq:sigma}{{7}{11}}
\MT@newlabel{eq:rho_tilde}
\MT@newlabel{eq:rho_hat}
\citation{Collman2015}
\citation{Collman2015}
\citation{vonLuxburg2007}
\citation{Tibshirani2001}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Clustering data from Fig.\nobreakspace  {}5\hbox {}a--c. }}{12}}
\newlabel{table:other}{{1}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Clustering MNIST data from Fig.\nobreakspace  {}5\hbox {}d. }}{12}}
\newlabel{table:mnist}{{2}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Real Data Experiment}{12}}
\MT@newlabel{eq:rho_alpha}
\MT@newlabel{eq:rho_hat}
\MT@newlabel{eq:kernel_matrix}
\MT@newlabel{eq:kernel_semimetric}
\@writefile{toc}{\contentsline {section}{\numberline {8}Discussion}{12}}
\newlabel{sec:conclusion}{{8}{12}}
\bibstyle{unsrt}
\bibdata{biblio.bib}
\bibcite{Szkely2013}{1}
\bibcite{RizzoVariance}{2}
\bibcite{RizzoClustering}{3}
\bibcite{Kgroups}{4}
\bibcite{Lyons}{5}
\bibcite{Sejdinovic2013}{6}
\bibcite{Lloyd}{7}
\bibcite{MacQueen}{8}
\bibcite{Forgy}{9}
\bibcite{Smola}{10}
\bibcite{Girolami}{11}
\bibcite{Mercer}{12}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Features of the synapse dataset according to \cite  {Collman2015}. }}{13}}
\newlabel{table:synapse}{{3}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Clustering synapse dataset. (a) Eigenvalues of random walk Laplacian. We choose $k=6$ since its the first time we see a peak. We use energy clustering with the two metrics $\rho _1$ and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \rho $}\mathaccent "0362{\rho }_2$. (b) Heat map of the cluster means versus features using energy clustering with standard metric $\rho _1$ from energy statistics. (c) Clustered points projected into the two principal components, using $\rho _1$. (d) Heat map of the cluster means versus features using energy clustering with Gaussian metric $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \rho $}\mathaccent "0362{\rho }_2$. (e) Clustered points projected into the two principal components, using $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \rho $}\mathaccent "0362{\rho }_2$. }}{13}}
\newlabel{fig:synapse}{{6}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces   Comparison of energy clustering algorithms to $k$-means and GMM on unbalanced clusters. The data is normally distributed as \MT_extended_eqref:n  {eq:gauss3}, where we vary $m \in [0, 240]$, and in each case we do $100$ Monte Carlo runs showing the average accuracy with standard error. }}{13}}
\newlabel{fig:unbalanced}{{7}{13}}
\MT@newlabel{eq:gauss3}
\newlabel{eq:gauss3}{{40}{13}}
\@writefile{toc}{\contentsline {section}{References}{13}}
\bibcite{Dhillon2}{13}
\bibcite{Dhillon}{14}
\bibcite{Filippone}{15}
\bibcite{Hartigan}{16}
\bibcite{Telgarsky}{17}
\bibcite{Slonin}{18}
\bibcite{Aronszajn}{19}
\bibcite{Gretton2012}{20}
\bibcite{Berg1984}{21}
\bibcite{Malik}{22}
\bibcite{NgJordan}{23}
\bibcite{Kernighan}{24}
\bibcite{Chan}{25}
\bibcite{Yu}{26}
\bibcite{scikit-learn}{27}
\bibcite{Vassilvitskii}{28}
\bibcite{Sapiro}{29}
\bibcite{Collman2015}{30}
\bibcite{vonLuxburg2007}{31}
\bibcite{Tibshirani2001}{32}
\@writefile{toc}{\contentsline {section}{Biographies}{14}}
\@writefile{toc}{\contentsline {subsection}{Guilherme Fran\c ca}{14}}
\@writefile{toc}{\contentsline {subsection}{Maria Rizzo}{14}}
\@writefile{toc}{\contentsline {subsection}{Joshua Vogelstein}{14}}
