\relax 
\citation{Szkely2013,Szkely2017}
\citation{RizzoVariance}
\citation{Szekely2007-mm}
\citation{RizzoClustering}
\citation{Szkely2013,Szkely2017}
\citation{Kgroups}
\citation{Lyons}
\citation{Sejdinovic2013}
\citation{Shen2018-st}
\citation{Lloyd,MacQueen,Forgy}
\citation{Lloyd}
\citation{Smola,Girolami}
\citation{Mercer}
\citation{Girolami}
\citation{Dhillon2,Dhillon}
\citation{Filippone}
\citation{Hartigan1975,Hartigan1979}
\citation{Telgarsky}
\citation{Telgarsky,Slonin}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{sec:introduction}{{1}{1}}
\citation{Kgroups}
\citation{Kgroups}
\citation{Szkely2013}
\citation{Lyons,Sejdinovic2013}
\citation{Szkely2013}
\citation{Sejdinovic2013}
\citation{Aronszajn}
\@writefile{toc}{\contentsline {section}{\numberline {2}Review of Energy Statistics and RKHS}{2}}
\newlabel{sec:background}{{2}{2}}
\newlabel{eq:energy}{{1}{2}}
\newlabel{eq:energy2}{{2}{2}}
\newlabel{eq:negative_type}{{3}{2}}
\newlabel{eq:energy3}{{4}{2}}
\citation{Gretton2012}
\citation{Berg1984}
\citation{Sejdinovic2013}
\citation{Sejdinovic2013}
\citation{Szkely2013}
\citation{Szkely2013}
\newlabel{eq:mmd}{{5}{3}}
\newlabel{eq:mmd2}{{6}{3}}
\newlabel{eq:kernel_semimetric}{{7}{3}}
\newlabel{eq:gen_kernel}{{8}{3}}
\newlabel{eq:Erho}{{9}{3}}
\newlabel{eq:g_def}{{10}{3}}
\newlabel{eq:within}{{11}{3}}
\newlabel{eq:between}{{12}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}The Clustering Problem Formulation}{3}}
\newlabel{sec:clustering_theory}{{3}{3}}
\newlabel{eq:g_def2}{{13}{3}}
\newlabel{eq:within2}{{15}{3}}
\newlabel{eq:between2}{{16}{3}}
\newlabel{th:minimize}{{1}{3}}
\newlabel{eq:minimize}{{17}{3}}
\newlabel{eq:kernel_matrix}{{19}{4}}
\newlabel{eq:label_matrix}{{20}{4}}
\newlabel{eq:weighted_matrices}{{21}{4}}
\newlabel{th:qcqp3}{{2}{4}}
\newlabel{eq:qcqp3}{{22}{4}}
\newlabel{eq:W2}{{23}{4}}
\newlabel{eq:max_prob}{{24}{4}}
\citation{Malik,NgJordan}
\citation{Kernighan,Malik,Chan,Yu}
\citation{Dhillon2,Dhillon}
\citation{Dhillon2,Dhillon}
\newlabel{eq:relaxed}{{29}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Connection with Graph Partitioning}{5}}
\newlabel{eq:assoc}{{31}{5}}
\newlabel{eq:cut}{{32}{5}}
\newlabel{eq:metric_graphs}{{35}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Connection with Kernel k-Means}{5}}
\newlabel{sec:kernel_kmeans}{{3.2}{5}}
\newlabel{eq:muj}{{37}{5}}
\newlabel{eq:kernel_kmeans}{{40}{5}}
\citation{Dhillon2,Dhillon}
\citation{Lloyd}
\citation{Hartigan1975,Hartigan1979}
\@writefile{toc}{\contentsline {section}{\numberline {4}Iterative Algorithms}{6}}
\newlabel{sec:algo}{{4}{6}}
\newlabel{eq:maxQ}{{41}{6}}
\newlabel{eq:costxij}{{42}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Weighted Kernel k-Means Algorithm}{6}}
\newlabel{eq:min_lloyd}{{43}{6}}
\newlabel{eq:Jell}{{44}{6}}
\newlabel{eq:min_lloyd2}{{45}{6}}
\newlabel{eq:closest_mean}{{46}{6}}
\newlabel{kmeans_algo}{{1}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces  Weighted version of kernel k-means algorithm to find local solutions to the optimization problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 22\hbox {}\unskip \@@italiccorr )}}. }}{6}}
\newlabel{algo}{{2}{6}}
\newlabel{stepmove}{{4.2}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces  Kernel k-groups algorithm, based on Hartigan's method, to find local solutions to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 22\hbox {}\unskip \@@italiccorr )}}. }}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Kernel k-Groups Algorithm}{6}}
\newlabel{eq:deltaQ}{{47}{6}}
\citation{Kgroups}
\citation{scikit-learn}
\citation{Vassilvitskii}
\citation{Malik}
\newlabel{eq:Qplus}{{48}{7}}
\newlabel{eq:Qminus}{{49}{7}}
\newlabel{eq:changeQ}{{50}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Experiments}{7}}
\newlabel{sec:numerics}{{5}{7}}
\newlabel{eq:rho_alpha}{{52}{7}}
\newlabel{eq:rho_tilde}{{53}{7}}
\newlabel{eq:rho_hat}{{54}{7}}
\newlabel{eq:accuracy}{{55}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Synthetic Experiments}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces   Accuracy results for the density estimation of Fig.\nobreakspace  {}1\hbox {}d--e. }}{7}}
\newlabel{tb:dens}{{1}{7}}
\newlabel{eq:gauss1}{{56}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   Clustering one-dimensional data for a two-class problem. (a) Data normally distributed as $x \mathrel {\mathop {\sim }\limits ^{iid}} (1/2) \mathcal  {N}(0, 1.5) + (1/2) \mathcal  {N}(1.5,0.3)$. (b) Data following lognormal distributions as $x \mathrel {\mathop {\sim }\limits ^{iid}} (1/2)e^{\mathcal  {N}(0, 1.5)} + (1/2) e^{\mathcal  {N}(1.5, 0.3)}$. In both cases we plot the average accuracy versus the total number of points (error bars are too small to be visible). (c) For the same distribution as in item (a), we sample 2000 points, cluster them with the three methods, then perform a kernel density estimation for kernel k-groups, since this is a model-free method, while for k-means and GMM we show the estimated Gaussian for each class. The clustering accuracy for each method is in Table\nobreakspace  {}1\hbox {}. (d) Exactly the same experiment but for the distribution in item (b). Note that only kernel k-groups is able to distinguish between the two classes in this example. }}{8}}
\newlabel{fig:1dexperiment}{{1}{8}}
\newlabel{eq:gauss2}{{57}{8}}
\newlabel{eq:20gauss}{{58}{8}}
\newlabel{eq:gauss3}{{59}{8}}
\citation{Dua2017,Guvenir1998}
\citation{RizzoClustering}
\citation{Dua2017,Guvenir1998}
\citation{RizzoClustering}
\citation{Kgroups}
\citation{Dua2017,Guvenir1998}
\citation{Dua2017,Guvenir1998}
\citation{RizzoClustering}
\citation{RizzoClustering}
\citation{Kgroups}
\citation{RizzoClustering}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   Scatter plot of the last two dimensions where $\mu _2$ has signal. Each plot has 200 points total. (a) Data distributed as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 56\hbox {}\unskip \@@italiccorr )}}. (b) Data distributed as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 57\hbox {}\unskip \@@italiccorr )}}. (c) Data distributed as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 58\hbox {}\unskip \@@italiccorr )}}. (d) Parameters as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 58\hbox {}\unskip \@@italiccorr )}} but for lognormal mixture. (e) Data from \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 59\hbox {}\unskip \@@italiccorr )}} with $N = 300$ and $m=200$. }}{9}}
\newlabel{fig:scatter}{{2}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Clustering results associated to the data illustrated in Fig.\nobreakspace  {}2\hbox {}. For each experiment we perform 100 Monte Carlo runs and show the average accuracy. We ommit error bars since they are too small to be visible. (a) High dimensional Gaussian mixture according to \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 56\hbox {}\unskip \@@italiccorr )}}. The dashed line is Bayes accuracy $\approx 0.86$. We use the metric $\rho _1$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 52\hbox {}\unskip \@@italiccorr )}}, which is standard in energy statistics. (b) High dimensional Gaussian mixture according to \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 57\hbox {}\unskip \@@italiccorr )}}. Bayes accuracy $\approx 0.95$. We use $\rho _1$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 52\hbox {}\unskip \@@italiccorr )}}. (c) Gaussian mixture with parameters \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 58\hbox {}\unskip \@@italiccorr )}}. We increase the number of sampled points in each trial. We use different metrics; see \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 52\hbox {}\unskip \@@italiccorr )}}--\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 54\hbox {}\unskip \@@italiccorr )}}. Here, kernel k-groups is more accurate than spectral clustering. (d) Same experiment as in Fig.\nobreakspace  {}3\hbox {}c but with a lognormal mixture with parameters \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 58\hbox {}\unskip \@@italiccorr )}}. Again, kernel k-groups is more accurate than alternatives. The plot suggests that neither of these methods are consistent on this example since Bayes accuracy is $\approx 0.90$. (e) Comparison between clustering methods on unbalanced clusters. The data is normally distributed as \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 59\hbox {}\unskip \@@italiccorr )}} where we vary $m \in [0, 240]$. We use the standard metric $\rho _1$ (see \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 52\hbox {}\unskip \@@italiccorr )}}) from energy statistics. }}{9}}
\newlabel{fig:plots}{{3}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Real Data Experiment}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   (a) Data distributed as $x \mathrel {\mathop {\sim }\limits ^{iid}} (1/2)\mathcal  {N}(\mu _1,\Sigma _1)+ (1/2)\mathcal  {N}(\mu _1,\Sigma _1)$, $\mu _1= (0,0)^\top $, $\mu _2 = (6.5, 0)^\top $ and $\Sigma _1 = \Sigma _2 = \diag  (1, 20)$. We sample $800$ points. (b) Concentric circles with radius $r_1 = 1$ and $r_2 = 3$, with noise $0.2 \cdot \mathcal  {N}(0, I_2)$. We sample 800 points with probability $1/2$ for each class. }}{9}}
\newlabel{fig:2dscatter}{{4}{9}}
\citation{energy}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces   Clustering the data from Fig.\nobreakspace  {}4\hbox {}. For spectral clustering, kernel k-means and kernel k-groups we use the metric $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \rho $}\mathaccent "0365{\rho }_2$ (see \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 53\hbox {}\unskip \@@italiccorr )}}) for the data in Fig.\nobreakspace  {}4\hbox {}a, while $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \rho $}\mathaccent "0362{\rho }_1$ (see \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 54\hbox {}\unskip \@@italiccorr )}}) for the data in Fig.\nobreakspace  {}4\hbox {}b. We have 800 points on each trial and 30 Monte Carlo runs for both datasets. }}{10}}
\newlabel{tb:cigar_circle}{{2}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces   Clustering the dermatology dataset of \cite  {Dua2017,Guvenir1998} with kernel k-groups using the metric $\rho _{1/2}$ (see \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 52\hbox {}\unskip \@@italiccorr )}}) from energy statistics. The table below should be compared with Table\nobreakspace  {}2 of \cite  {RizzoClustering}, for which our results are slightly more accurate. See also Table\nobreakspace  {}4\hbox {} below for clustering metrics. The classes in the vertical indicates the ground truth and the classes in the horizontal correspond to the classification obtained by kernel k-groups. We show the estimated number of points for each class. }}{10}}
\newlabel{tb:dermatology}{{3}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{10}}
\newlabel{sec:conclusion}{{6}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces   For the dataset \cite  {Dua2017,Guvenir1998} (see also Table\nobreakspace  {}3\hbox {}) we show the accuracy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 55\hbox {}\unskip \@@italiccorr )}} and the adjusted Rand index (aRand) of several methods. In \cite  {RizzoClustering} the authors obtained $\textnormal  {aRand}=0.9195$ using an energy statistics based method, while \cite  {Kgroups} obtains $\textnormal  {aRand}=0.9188$ where points with missing entries are removed. Below we complete the missing entries with the mean. If we remove the points with missing entries, kernel k-groups provides an improvement of $\textnormal  {accuracy}=0.9637$ and $\textnormal  {aRand}=0.9396$. }}{10}}
\newlabel{tb:dermatology_accuracy}{{4}{10}}
\@writefile{toc}{\contentsline {section}{Appendix}{10}}
\newlabel{sec:twoclass}{{A}{10}}
\newlabel{eq:g_ind}{{61}{10}}
\newlabel{eq:g1d}{{62}{10}}
\bibstyle{unsrt}
\bibdata{biblio.bib}
\bibcite{Szkely2013}{1}
\bibcite{Szkely2017}{2}
\bibcite{RizzoVariance}{3}
\bibcite{Szekely2007-mm}{4}
\bibcite{RizzoClustering}{5}
\bibcite{Kgroups}{6}
\bibcite{Lyons}{7}
\bibcite{Sejdinovic2013}{8}
\bibcite{Shen2018-st}{9}
\bibcite{Lloyd}{10}
\bibcite{MacQueen}{11}
\bibcite{Forgy}{12}
\bibcite{Smola}{13}
\bibcite{Girolami}{14}
\bibcite{Mercer}{15}
\bibcite{Dhillon2}{16}
\bibcite{Dhillon}{17}
\bibcite{Filippone}{18}
\bibcite{Hartigan1975}{19}
\bibcite{Hartigan1979}{20}
\bibcite{Telgarsky}{21}
\bibcite{Slonin}{22}
\bibcite{Aronszajn}{23}
\bibcite{Gretton2012}{24}
\bibcite{Berg1984}{25}
\bibcite{Malik}{26}
\bibcite{NgJordan}{27}
\bibcite{Kernighan}{28}
\bibcite{Chan}{29}
\bibcite{Yu}{30}
\bibcite{scikit-learn}{31}
\bibcite{Vassilvitskii}{32}
\newlabel{eq:w1d}{{63}{11}}
\newlabel{algo1d}{{3}{11}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces   Clustering algorithm to find local solutions to the optimization problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 17\hbox {}\unskip \@@italiccorr )}} for a two-class problem in one dimension. }}{11}}
\@writefile{toc}{\contentsline {section}{References}{11}}
\bibcite{Dua2017}{33}
\bibcite{Guvenir1998}{34}
\bibcite{energy}{35}
\@writefile{toc}{\contentsline {section}{Biographies}{12}}
\@writefile{toc}{\contentsline {subsection}{Guilherme Fran\c ca}{12}}
\@writefile{toc}{\contentsline {subsection}{Maria Rizzo}{12}}
\@writefile{toc}{\contentsline {subsection}{Joshua\nobreakspace  {}T.\nobreakspace  {}Vogelstein}{12}}
